{
  "name": "Csv loader for redshift",
  "tagline": "Loads CSV file to Amazon-Redshift table from Windows CLI",
  "body": "# CSV File Loader for Amazon Redshift DB.\r\nLoads CSV file to Amazon-Redshift table from Windows command line.\r\n\r\nFeatures:\r\n - Loads local (to your Windows desktop) CSV file to Amazon Redshift.\r\n - No need to preload your data to S3 prior to insert to Redshift.\r\n - No need for Amazon AWS CLI.\r\n - Works from your OS Windows desktop (command line).\r\n - It's executable (csv_loader_for_redshift.exe)  - no need for Python install.\r\n - It's 32 bit - it will work on any vanilla Windows.\r\n - AWS Access Keys are not passed as arguments. \r\n - Written using Python/boto/PyInstaller.\r\n - \r\n\r\n##Version\r\n\r\nOS|Platform|Version \r\n---|---|---- | -------------\r\nWindows|64bit|[0.1.0 beta]\r\n\r\n##Purpose\r\n\r\n- Ad-hoc CSV file load to Amazon Redshift table.\r\n\r\n## How it works\r\n- File is staged on S3 prior to load to Redshift\r\n- Optional upload to Reduced Redundancy storage (not RR by default).\r\n- Optional \"make it public\" after upload (private by default)\r\n- S3 Key defaulted to transfer file name.\r\n- Load is done using COPY command\r\n- Target Redshift table has to exist\r\n- It's a Python/boto/psycopg2 script\r\n\t* Boto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n\t* psycopg2 docs: http://initd.org/psycopg/docs/\r\n- Executable is created using [pyInstaller] (http://www.pyinstaller.org/)\r\n\r\n##Audience\r\n\r\nDatabase/ETL developers, Data Integrators, Data Engineers, Business Analysts, AWS Developers, DevOps, \r\n\r\n##Designated Environment\r\nPre-Prod (UAT/QA/DEV)\r\n\r\n##Usage\r\n\r\n```\r\n## Load CSV file to Amazon Redshift table.\r\n##\r\n## Load % progress outputs to the screen.\r\n##\r\nUsage:  \r\n  set AWS_ACCESS_KEY_ID=<you access key>\r\n  set AWS_SECRET_ACCESS_KEY=<you secret key>\r\n  set REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n  csv_loader_for_redshift.py <file_to_transfer> <bucket_name> [<use_rr>] [<public>]\r\n\t\t\t\t\t\t [<delim>] [<quote>] [<to_table>] [<gzip_source_file>]\r\n\t\r\n\t--use_rr -- Use reduced redundancy storage (False).\r\n\t--public -- Make uploaded files public (False).\r\n\t--delim  -- CSV file delimiter (',').\r\n\t--quote  -- CSV quote ('\"').\r\n\t--to_table  -- Target Amazon-Redshit table name.\r\n\t--gzip_source_file  -- gzip input CVS file before upload to Amazon-S3 (False).\r\n\t\r\n\tInput filename will be used for S3 key name.\r\n\t\r\n\tBoto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n\tpsycopg2 docs: http://initd.org/psycopg/docs/\r\n\t\r\n\"\"\"\r\n\r\n```\r\n#Example\r\n\r\n\r\n###Environment variables\r\n\r\n* Set the following environment variables (for all tests:\r\n\r\n```\r\nset AWS_ACCESS_KEY_ID=<you access key>\r\nset AWS_SECRET_ACCESS_KEY=<you secret key>\r\n\r\nset REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n```\r\n\r\n### CSV file upload into Redshift table `test2`\r\n\r\n\r\n* examples\\Load_CSV_To_Redshift_Table.bat\r\n```\r\nset AWS_ACCESS_KEY_ID=<you access key>\r\nset AWS_SECRET_ACCESS_KEY=<you secret key>\r\nset REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n  \r\ncd c:\\tmp\\CSV_Loader\r\ncsv_loader_for_redshift.exe c:\\tmp\\data.csv test123 -r -d \",\" -t test2 -z\r\n\r\n```\r\n* resutl.log (Load_CSV_To_Redshift_Table.bat > resutl.log)\r\n```\r\nS3        | data.csv.gz | 100%\r\nRedshift  | test2       | DONE\r\nTime elapsed: 5.7 seconds\r\n\r\n```\r\n##Test prerequisits.\r\n\r\n####Target Redshift table DDL\r\n\r\n```\r\nCREATE TABLE test2 (id integer , num integer, data varchar,num2 integer, data2 varchar,num3 \r\ninteger, data3 varchar,num4 integer, data4 varchar);\r\n\r\n```\r\n\r\n####Test data\r\n* Test data is in file examples\\data.csv\r\n\r\n###Sources\r\n* Will add as soon as I clean em up and remove all the passwords and AWS keys :-)).\r\n\r\n###Download\r\n* `git clone https://github.com/alexbuz/CSV_Loader_For_Redshift`\r\n* [Master Release](https://github.com/alexbuz/CSV_Loader_For_Redshift/archive/master.zip) -- `csv_loader_for_redshift 0.1.0`\r\n\r\n\r\n\r\n#   \r\n#FAQ\r\n#  \r\n#### Can it load CSV file from Windows desktop to Amazon Redshift.\r\nYes. This is the main purpose of the `CSV Loader for Redshift`.\r\n\r\n#### Can developers integrate CSV loader into their ETL pipelines?\r\nYes. Assuming they are doing it on OS Windows.\r\n\r\n#### How fast is data upload using `CSV Loader for Redshift`?\r\nAs fast as any AWS API provided by Amazon.\r\n\r\n####How to inscease upload speed?\r\nCompress input file or provide `-z` or `--gzip_source_file` arg in command line and this tool will compress it for you before upload to S3.\r\n\r\n#### What are the other ways to upload file to Redshift?\r\nYou can use 'aws s3api' and psql COPY command to do pretty much the same.\r\n\r\n#### Can I just zip it using Windows File Explorer?\r\nNo, Redshift will not recognize *.zip file format.\r\nYou have to `gzip` it. You can use 7-Zip to do that.\r\n\r\n\r\n#### Does it delete file from S3 after upload?\r\nNo\r\n\r\n#### Does it create target Redshift table?\r\nNo\r\n\r\n#### Is there an option to compress input CSV file before upload?\r\nYes. Use `-z` or `--gzip_source_file` argument so the tool does compression for you.\r\n\r\n\r\n#### Explain first step of data load?\r\nThe CSV you provided is getting preloaded to Amazon-S3.\r\nIt doesn't have to be made public for load to Redshift. \r\nIt can be compressed or uncompressed.\r\nYour input file is getting compressed (optional) and uploaded to S3 using credentials you set in shell.\r\n\r\n\r\n#### Explain second step of data load. How data is loaded to Amazon Redshift?\r\nYou Redshift cluster has to be open to the world (accessible via port 5439 from internet).\r\nIt uses PostgreSQL COPY command to load file located on S3 into Redshift table.\r\n\r\n\r\n#### Can I use WinZip or 7-zip\r\nYes, but you have to use 'gzip' compression type.\r\n\r\n#### What technology was used to create this tool\r\nI used Python, Boto, and psycopg2 to write it.\r\nBoto is used to upload file to S3. \r\npsycopg2 is used to establish ODBC connection with Redshift clusted and execute `COPY` command.\r\n\r\n#### Where are the sources?\r\nPlease, contact me for sources.\r\n\r\n#### Can you modify functionality and add features?\r\nYes, please, ask me for new features.\r\n\r\n#### Can you create similar/custom data tool for our business?\r\nYes, you can PM me here or email at `alex_buz@yahoo.com`.\r\nI'll get back to you within hours.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}