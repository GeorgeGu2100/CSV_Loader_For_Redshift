{
  "name": "Csv loader for redshift",
  "tagline": "Loads CSV file to Amazon-Redshift table from Windows CLI",
  "body": "# CSV File Loader for Amazon Redshift DB.\r\nSimple CSV file to Amazon-Redshift table loader.\r\n- Input file has to be located on your Windows desktop (not S3).\r\n- Execute loader from Windows CLI (cmd/command line)\r\n\r\n\r\n##Version\r\n\r\nOS|Platform|Version \r\n---|---|---- | -------------\r\nWindows|64bit|[0.1.0 beta]\r\n\r\n##Purpose\r\n\r\n- Ad-hoc CSV file load to Amazon Redshift table.\r\n\r\n## How it works\r\n- File is staged on S3 prior to load to Redshift\r\n- Optional upload to Reduced Redundancy storage (not RR by default).\r\n- Optional \"make it public\" after upload (private by default)\r\n- S3 Key defaulted to transfer file name.\r\n- Load is done using COPY command\r\n- Target Redshift table has to exist\r\n- It's a Python/boto/psycopg2 script\r\n\t* Boto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n\t* psycopg2 docs: http://initd.org/psycopg/docs/\r\n- Executable is created using [pyInstaller] (http://www.pyinstaller.org/)\r\n\r\n##Audience\r\n\r\nDatabase/ETL developers, Data Integrators, Data Engineers, Business Analysts, AWS Developers, DevOps, \r\n\r\n##Designated Environment\r\nPre-Prod (UAT/QA/DEV)\r\n\r\n##Usage\r\n\r\n```\r\n## Load CSV file to Amazon Redshift table.\r\n##\r\n## Load % progress outputs to the screen.\r\n##\r\nUsage:  \r\n  set AWS_ACCESS_KEY_ID=<you access key>\r\n  set AWS_SECRET_ACCESS_KEY=<you secret key>\r\n  set REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n  csv_loader_for_redshift.py <file_to_transfer> <bucket_name> [<use_rr>] [<public>]\r\n\t\t\t\t\t\t [<delim>] [<quote>] [<to_table>] [<gzip_source_file>]\r\n\t\r\n\t--use_rr -- Use reduced redundancy storage (False).\r\n\t--public -- Make uploaded files public (False).\r\n\t--delim  -- CSV file delimiter (',').\r\n\t--quote  -- CSV quote ('\"').\r\n\t--to_table  -- Target Amazon-Redshit table name.\r\n\t--gzip_source_file  -- gzip input CVS file before upload to Amazon-S3 (False).\r\n\t\r\n\tInput filename will be used for S3 key name.\r\n\t\r\n\tBoto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n\tpsycopg2 docs: http://initd.org/psycopg/docs/\r\n\t\r\n\"\"\"\r\n\r\n```\r\n\r\n##Environment variables\r\n\r\n* Set the following environment variables:\r\n\r\n```\r\nset AWS_ACCESS_KEY_ID=<you access key>\r\nset AWS_SECRET_ACCESS_KEY=<you secret key>\r\n\r\nset REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n```\r\n\r\n##Example file load into Redshift table `test2`\r\n\r\n\r\n* examples\\Load_CSV_To_Redshift_Table.bat\r\n```\r\nset AWS_ACCESS_KEY_ID=<you access key>\r\nset AWS_SECRET_ACCESS_KEY=<you secret key>\r\nset REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n  \r\ncd c:\\tmp\\CSV_Loader\r\ncsv_loader_for_redshift.exe c:\\tmp\\data.csv test123 -r -d \",\" -t test2 -z\r\n\r\n```\r\n* resutl.log (Load_CSV_To_Redshift_Table.bat > resutl.log)\r\n```\r\nS3        | data.csv.gz | 100%\r\nRedshift  | test2       | DONE\r\nTime elapsed: 5.7 seconds\r\n\r\n```\r\n\r\n###Target Redshift table DDL\r\n\r\n```\r\nCREATE TABLE test2 (id integer , num integer, data varchar,num2 integer, data2 varchar,num3 \r\ninteger, data3 varchar,num4 integer, data4 varchar);\r\n\r\n```\r\n\r\n###Test data\r\n* Test data is in file examples\\data.csv\r\n\r\n##Sources\r\n* Will add as soon as I clean em up and remove all the passwords and AWS keys :-)).\r\n\r\n##Download\r\n* `git clone https://github.com/alexbuz/CSV_Loader_For_Redshift`\r\n* [Master Release](https://github.com/alexbuz/CSV_Loader_For_Redshift/archive/master.zip) -- `csv_loader_for_redshift 0.1.0`\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}